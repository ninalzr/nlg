{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BasicLSTMEncoderDecoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN9VU16jwak9o82BJ7Mk/N1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ninalzr/nlg/blob/master/BasicLSTMEncoderDecoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xju3aW_Uirp",
        "colab_type": "code",
        "outputId": "6bf38682-3bdb-4e24-8553-820d059df9b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.86)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0vtYBwRwnM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, sys, json\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJbeG0HrWoA_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Lookup():\n",
        "    def __init__(self, model_class, file_prefix = None):\n",
        "\n",
        "        self.model_class = model_class\n",
        "\n",
        "        self.bos_token = None\n",
        "        self.eos_token = None\n",
        "        self.unk_token = None\n",
        "        self.sep_token = None\n",
        "        self.pad_token = None\n",
        "        self.cls_token = None\n",
        "        self.mask_token = None\n",
        "\n",
        "        if model_class == 'gpt2':\n",
        "            from transformers import GPT2Tokenizer\n",
        "            self._tokenizer = GPT2Tokenizer.from_pretrained(model_class)\n",
        "            self._tokenizer.add_special_tokens({'pad_token': '<PAD>'})\n",
        "            if self._tokenizer._bos_token:\n",
        "                self.bos_token = self._tokenizer.bos_token\n",
        "            if self._tokenizer._eos_token:\n",
        "                self.eos_token = self._tokenizer.eos_token\n",
        "            if self._tokenizer._unk_token:                \n",
        "                self.unk_token = self._tokenizer.unk_token\n",
        "            if self._tokenizer._sep_token:\n",
        "                self.sep_token = self._tokenizer.sep_token\n",
        "            if self._tokenizer._pad_token:\n",
        "                self.pad_token = self._tokenizer.pad_token\n",
        "            if self._tokenizer._cls_token:\n",
        "                self.cls_token = self._tokenizer.cls_token\n",
        "            if self._tokenizer._mask_token:\n",
        "                self.mask_token = self._tokenizer.mask_token \n",
        "        else:\n",
        "            print(\"You need to load a tokenizer from https://huggingface.co/transformers/main_classes/tokenizer.html#\")\n",
        "        \n",
        "        if file_prefix:\n",
        "            self.load(file_prefix)\n",
        "\n",
        "        def save_special_tokens(self, file_prefix):\n",
        "            if self.model_class == \"gpt2\":\n",
        "                special_tokens = {}\n",
        "            if self.bos_token:\n",
        "                special_tokens['bos_token'] = self.bos_token\n",
        "            if self.eos_token:\n",
        "                special_tokens['eos_token'] = self.eos_token\n",
        "            if self.unk_token:\n",
        "                special_tokens['unk_token'] = self.unk_token\n",
        "            if self.sep_token:\n",
        "                special_tokens['sep_token'] = self.sep_token\n",
        "            if self.pad_token:\n",
        "                special_tokens['pad_token'] = self.pad_token\n",
        "            if self.cls_token:\n",
        "                special_tokens['cls_token'] = self.cls_token\n",
        "            if self.mask_token:\n",
        "                special_tokens['mask_token'] = self.mask_token            \n",
        "            json.dump(special_tokens, open(file_prefix+\".special_tokens\",\"w\",encoding=\"utf8\"), indent=4, sort_keys=True)            \n",
        "            self._tokenizer.add_special_tokens(special_tokens)  \n",
        "        \n",
        "        def load(self, file_prefix):\n",
        "            if os.path.exists(file_prefix+\".special_tokens\"):\n",
        "                special_tokens = json.load(open(file_prefix+\".special_tokens\",\"r\",encoding=\"utf8\"))            \n",
        "            if 'bos_token' in special_tokens:\n",
        "                self.bos_token = special_tokens['bos_token']\n",
        "            if 'eos_token' in special_tokens:\n",
        "                self.eos_token = special_tokens['eos_token']\n",
        "            if 'unk_token' in special_tokens:\n",
        "                self.unk_token = special_tokens['unk_token']\n",
        "            if 'sep_token' in special_tokens:\n",
        "                self.sep_token = special_tokens['sep_token']\n",
        "            if 'pad_token' in special_tokens:\n",
        "                self.pad_token = special_tokens['pad_token']\n",
        "            if 'cls_token' in special_tokens:\n",
        "                self.cls_token = special_tokens['cls_token']\n",
        "            if 'mask_token' in special_tokens:\n",
        "                self.mask_token = special_tokens['mask_token']\n",
        "            self._tokenizer.add_special_tokens(special_tokens)      \n",
        "\n",
        "    def tokenize(self, text):\n",
        "        return self._tokenizer.tokenize(text)\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "        return self._tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    def convert_ids_to_tokens(self, token_ids):\n",
        "        return self._tokenizer.convert_ids_to_tokens(token_ids)\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens):\n",
        "        return self._tokenizer.convert_tokens_to_string(tokens)\n",
        "\n",
        "    def encode(self, text, add_bos_eos_tokens = False):\n",
        "        tokens = self.tokenize(text)\n",
        "        if add_bos_eos_tokens:\n",
        "            return [self.convert_tokens_to_ids(self.bos_token)] + self.convert_tokens_to_ids(tokens) + [self.convert_tokens_to_ids(self.eos_token)]\n",
        "        else:\n",
        "            return self.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    def decode(self, token_ids, skip_bos_eos_tokens = False):\n",
        "        if skip_bos_eos_tokens:\n",
        "            if len(token_ids)>0:\n",
        "                if token_ids[0] == self.convert_tokens_to_ids(self.bos_token):\n",
        "                    token_ids = token_ids[1:]\n",
        "            if len(token_ids)>0:\n",
        "                if token_ids[-1] == self.convert_tokens_to_ids(self.eos_token):\n",
        "                    token_ids = token_ids[:-1]   \n",
        "        if len(token_ids) > 0:\n",
        "            tokens = self.convert_ids_to_tokens(token_ids)\n",
        "            return self.convert_tokens_to_string(tokens)\n",
        "        return \"\"\n",
        "\n",
        "    def __len__(self):          \n",
        "        return len(self._tokenizer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FrE54XBuEuv",
        "colab_type": "code",
        "outputId": "8a1de47b-3bf0-4e5c-c009-41e154a80511",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        }
      },
      "source": [
        "model = 'gpt2'\n",
        "lookup = Lookup(model)\n",
        "text = \"Daisy, Daisy, Give me your answer, do!\"\n",
        "print(\"\\n1. String to tokens (tokenize):\")\n",
        "tokens = lookup.tokenize(text)\n",
        "print(tokens)\n",
        "\n",
        "print(\"\\n2. Tokens to ints (convert_tokens_to_ids):\")\n",
        "ids = lookup.convert_tokens_to_ids(tokens)\n",
        "print(ids)\n",
        "\n",
        "print(\"\\n2.5 Token to int (convert_tokens_to_ids with a single str):\")\n",
        "id = lookup.convert_tokens_to_ids(tokens[0])\n",
        "print(id)\n",
        "\n",
        "print(\"\\n3. Ints to tokens (convert_ids_to_tokens):\")\n",
        "tokens = lookup.convert_ids_to_tokens(ids)\n",
        "print(tokens)\n",
        "\n",
        "print(\"\\n3.5 Int to token (convert_ids_to_tokens with a single int):\")\n",
        "token = lookup.convert_ids_to_tokens(id)\n",
        "print(token)\n",
        "\n",
        "print(\"\\n4. Tokens to string (convert_tokens_to_string):\")\n",
        "recreated_text = lookup.convert_tokens_to_string(tokens)\n",
        "print(recreated_text)\n",
        "\n",
        "print(\"\\n5. String to ints (encode):\")\n",
        "ids = lookup.encode(text)\n",
        "print(ids)\n",
        "\n",
        "print(\"\\n6. Ints to string (decode):\")\n",
        "recreated_text = lookup.decode(ids)\n",
        "print(recreated_text)\n",
        "\n",
        "print(\"\\n7. Encode adding special tokens:\")\n",
        "ids = lookup.encode(text, add_bos_eos_tokens=True)\n",
        "print(ids)\n",
        "print(\"How it looks like with tokens: {}\".format(lookup.convert_ids_to_tokens(ids)))\n",
        "    \n",
        "print(\"\\n8. Decode skipping special tokens:\")\n",
        "recreated_text = lookup.decode(ids, skip_bos_eos_tokens=True)\n",
        "print(recreated_text)\n",
        "\n",
        "print(\"\\n9. Vocabulary size:\")\n",
        "vocab_size = lookup.__len__()\n",
        "print(vocab_size)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "1. String to tokens (tokenize):\n",
            "['Da', 'isy', ',', 'ĠDaisy', ',', 'ĠGive', 'Ġme', 'Ġyour', 'Ġanswer', ',', 'Ġdo', '!']\n",
            "\n",
            "2. Tokens to ints (convert_tokens_to_ids):\n",
            "[26531, 13560, 11, 40355, 11, 13786, 502, 534, 3280, 11, 466, 0]\n",
            "\n",
            "2.5 Token to int (convert_tokens_to_ids with a single str):\n",
            "26531\n",
            "\n",
            "3. Ints to tokens (convert_ids_to_tokens):\n",
            "['Da', 'isy', ',', 'ĠDaisy', ',', 'ĠGive', 'Ġme', 'Ġyour', 'Ġanswer', ',', 'Ġdo', '!']\n",
            "\n",
            "3.5 Int to token (convert_ids_to_tokens with a single int):\n",
            "Da\n",
            "\n",
            "4. Tokens to string (convert_tokens_to_string):\n",
            "Daisy, Daisy, Give me your answer, do!\n",
            "\n",
            "5. String to ints (encode):\n",
            "[26531, 13560, 11, 40355, 11, 13786, 502, 534, 3280, 11, 466, 0]\n",
            "\n",
            "6. Ints to string (decode):\n",
            "Daisy, Daisy, Give me your answer, do!\n",
            "\n",
            "7. Encode adding special tokens:\n",
            "[50256, 26531, 13560, 11, 40355, 11, 13786, 502, 534, 3280, 11, 466, 0, 50256]\n",
            "How it looks like with tokens: ['<|endoftext|>', 'Da', 'isy', ',', 'ĠDaisy', ',', 'ĠGive', 'Ġme', 'Ġyour', 'Ġanswer', ',', 'Ġdo', '!', '<|endoftext|>']\n",
            "\n",
            "8. Decode skipping special tokens:\n",
            "Daisy, Daisy, Give me your answer, do!\n",
            "\n",
            "9. Vocabulary size:\n",
            "50258\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpY8TIfw05oy",
        "colab_type": "text"
      },
      "source": [
        "TODO: Adjust the loader for distributed training (maybe?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE4Xm_u50KLG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, sys, json, random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch as nn\n",
        "import torch.utils.data\n",
        "\n",
        "from functools import partial\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAhcaTi_2cjP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWNQ3_8_5oWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgCYVQsx6FvL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loader(data_folder, batch_size, src_lookup, tgt_lookup, min_seq_len_X = 5, max_seq_len_X = 1000, min_seq_len_y = 5,\n",
        "           max_seq_len_y = 1000, MEI = \"\"):\n",
        "    MEI = MEI.replace(\" \",\"_\")\n",
        "    pad_id = tgt_lookup.convert_tokens_to_ids(tgt_lookup.pad_token)\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        MyDataset(data_folder, \"train\", min_seq_len_X, max_seq_len_X, min_seq_len_y, max_seq_len_y, MEI),\n",
        "        num_workers=0,\n",
        "        batch_size=batch_size,\n",
        "        collate_fn=partial(paired_collate_fn, padding_idx = pad_id),\n",
        "        shuffle=True)\n",
        "\n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        MyDataset(data_folder, \"dev\", min_seq_len_X, max_seq_len_X, min_seq_len_y, max_seq_len_y, MEI),\n",
        "        num_workers=0,\n",
        "        batch_size=batch_size,\n",
        "        collate_fn=partial(paired_collate_fn, padding_idx = pad_id))\n",
        "    \n",
        "    return train_loader, valid_loader\n",
        "\n",
        "def paired_collate_fn(insts, padding_idx):\n",
        "    # insts contains a batch_size number of (x, y) elements    \n",
        "    src_insts, tgt_insts = list(zip(*insts))\n",
        "   \n",
        "    # now src is a batch_size(=64) array of x0 .. x63, and tgt is y0 .. x63 ; xi is variable length\n",
        "    # ex: if a = [(1,2), (3,4), (5,6)]\n",
        "    # then b, c = list(zip(*a)) => b = (1,3,5) and b = (2,4,6)\n",
        "    \n",
        "    # src_insts is now a tuple of batch_size Xes (x0, x63) where xi is an instance\n",
        "    #src_insts, src_lenghts, tgt_insts, tgt_lenghts = length_collate_fn(src_insts, tgt_insts)       \n",
        "    \n",
        "    src_max_len = max(len(inst) for inst in src_insts) # determines max size for all examples\n",
        "    \n",
        "    src_seq_lengths = torch.tensor(list(map(len, src_insts)), dtype=torch.long)    \n",
        "    src_seq_tensor = torch.tensor(np.array( [ inst + [padding_idx] * (src_max_len - len(inst)) for inst in src_insts ] ), dtype=torch.long)\n",
        "    src_seq_mask = torch.tensor(np.array( [ [1] * len(inst) + [0] * (src_max_len - len(inst)) for inst in src_insts ] ), dtype=torch.long)\n",
        "    \n",
        "    src_seq_lengths, perm_idx = src_seq_lengths.sort(0, descending=True)\n",
        "    src_seq_tensor = src_seq_tensor[perm_idx]   \n",
        "    src_seq_mask = src_seq_mask[perm_idx]\n",
        "    tgt_max_len = max(len(inst) for inst in tgt_insts)\n",
        "    \n",
        "    tgt_seq_lengths = torch.tensor(list(map(len, tgt_insts)), dtype=torch.long)    \n",
        "    tgt_seq_tensor = torch.tensor(np.array( [ inst + [padding_idx] * (tgt_max_len - len(inst)) for inst in tgt_insts ] ), dtype=torch.long)\n",
        "    tgt_seq_mask = torch.tensor(np.array( [ [1] * len(inst) + [0] * (tgt_max_len - len(inst)) for inst in tgt_insts ] ), dtype=torch.long)\n",
        "    \n",
        "    tgt_seq_lengths = tgt_seq_lengths[perm_idx]\n",
        "    tgt_seq_tensor = tgt_seq_tensor[perm_idx]      \n",
        "    tgt_seq_mask = tgt_seq_mask[perm_idx]   \n",
        "      \n",
        "    return ((src_seq_tensor, src_seq_lengths, src_seq_mask), (tgt_seq_tensor, tgt_seq_lengths, tgt_seq_mask))   \n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, type, min_seq_len_X, max_seq_len_X, min_seq_len_y, max_seq_len_y, MEI):  \n",
        "        self.root_dir = root_dir\n",
        "\n",
        "        self.X = [] # this will store joined sentences\n",
        "        self.y = [] # this will store the output\n",
        "\n",
        "    \n",
        "        with open(os.path.join(root_dir, type, MEI + '_output.txt'), 'r') as f:\n",
        "            y = [lookup.encode(y.strip(), add_bos_eos_tokens=True)  for y in f]\n",
        "        with open(os.path.join(root_dir, type, MEI + '_sentences.txt'), 'r') as g:\n",
        "            X = [lookup.encode(x.strip(), add_bos_eos_tokens=True)  for x in g]   \n",
        "\n",
        "        \n",
        "        cut_over_X = 0\n",
        "        cut_under_X = 0\n",
        "        cut_over_y = 0\n",
        "        cut_under_y = 0\n",
        "        \n",
        "        # max len\n",
        "        for (sx, sy) in zip(X, y):\n",
        "            if len(sx) > max_seq_len_X:\n",
        "                cut_over_X += 1\n",
        "            elif len(sx) < min_seq_len_X+2:                \n",
        "                cut_under_X += 1\n",
        "            elif len(sy) > max_seq_len_y:\n",
        "                cut_over_y += 1\n",
        "            elif len(sy) < min_seq_len_y+2:                \n",
        "                cut_under_y += 1\n",
        "            else:\n",
        "                self.X.append(sx)\n",
        "                self.y.append(sy)         \n",
        "\n",
        "        c = list(zip(self.X, self.y))\n",
        "        random.shuffle(c)\n",
        "        self.X, self.y = zip(*c)\n",
        "        self.X = list(self.X)\n",
        "        self.y = list(self.y)\n",
        "        print(X)\n",
        "        print(y)\n",
        "                    \n",
        "        print(\"Dataset [{}] loaded with {} out of {} ({}%) instances.\".format(type, len(self.X), len(X), float(100.*len(self.X)/len(X)) ) )\n",
        "        print(\"\\t\\t For X, {} are over max_len {} and {} are under min_len {}.\".format(cut_over_X, max_seq_len_X, cut_under_X, min_seq_len_X))\n",
        "        print(\"\\t\\t For y, {} are over max_len {} and {} are under min_len {}.\".format(cut_over_y, max_seq_len_y, cut_under_y, min_seq_len_y))\n",
        "        \n",
        "        assert(len(self.X)==len(self.y))\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):        \n",
        "        return self.X[idx], self.y[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lFiT64s2oqV",
        "colab_type": "code",
        "outputId": "bc4b6eb2-238f-4623-838b-336095d06ba4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_path = 'drive/My Drive/nlg/tiny'\n",
        "src_lookup = Lookup(model)\n",
        "tgt_lookup = Lookup(model)\n",
        "batch_size = 4    \n",
        "min_seq_len_X = 10\n",
        "max_seq_len_X = 1000\n",
        "min_seq_len_y = min_seq_len_X\n",
        "max_seq_len_y = max_seq_len_X \n",
        "MEI = \"Management Overview\"\n",
        "model = 'gpt2'\n",
        "lookup = Lookup(model)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aO6bDIvJ6buN",
        "colab_type": "code",
        "outputId": "73659cb6-8f81-4ba0-9e0b-42e5e14239a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "source": [
        "train_loader, valid_loader = loader(data_path, batch_size, src_lookup, tgt_lookup, min_seq_len_X, max_seq_len_X, min_seq_len_y, max_seq_len_y, MEI = MEI)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[50256, 1, 818, 2274, 812, 11, 262, 1664, 750, 407, 7715, 5981, 13380, 38, 3136, 13, 32, 3096, 5583, 379, 262, 1664, 318, 4497, 329, 29852, 18848, 2428, 691, 13, 464, 1664, 16523, 281, 6142, 2450, 13, 10493, 2370, 5644, 262, 1664, 857, 407, 423, 5423, 8998, 379, 1919, 5127, 6333, 2428, 13, 464, 1919, 5127, 6333, 3210, 16523, 4571, 319, 11149, 4137, 290, 1200, 10515, 13, 464, 1664, 338, 34875, 11383, 468, 12872, 5260, 526, 50256], [50256, 1546, 38, 6447, 379, 262, 1664, 318, 4939, 13, 464, 1664, 338, 5583, 4497, 329, 15030, 13380, 38, 2428, 318, 2174, 262, 3096, 1241, 13, 464, 1664, 338, 6142, 2450, 318, 1913, 13, 464, 1664, 468, 407, 4920, 5423, 284, 5698, 262, 4542, 286, 1919, 5127, 6333, 2428, 13, 464, 1919, 5127, 6333, 3210, 16523, 4571, 319, 11149, 4137, 290, 1200, 10515, 13, 464, 1664, 857, 407, 7271, 15771, 257, 34875, 11383, 13, 50256], [50256, 464, 1664, 16523, 287, 13380, 38, 13019, 355, 340, 468, 407, 3199, 5981, 3136, 287, 2274, 812, 13, 464, 1664, 468, 407, 9899, 3096, 1241, 15662, 329, 13380, 38, 2428, 13, 464, 6142, 2450, 318, 4939, 13, 20636, 5127, 6333, 5423, 389, 407, 287, 1295, 379, 262, 1664, 13, 464, 5127, 6333, 5423, 466, 407, 2209, 1200, 10515, 290, 4137, 10515, 13, 32, 4939, 34875, 11383, 318, 287, 779, 379, 262, 1664, 13, 50256], [50256, 1, 818, 2274, 812, 11, 262, 1664, 750, 407, 7715, 5981, 13380, 38, 3136, 13, 464, 1664, 468, 407, 9899, 3096, 1241, 15662, 329, 13380, 38, 2428, 13, 15001, 319, 7271, 1695, 2370, 11, 262, 1664, 857, 407, 423, 257, 2450, 13593, 262, 2858, 13, 464, 1664, 468, 407, 4920, 5423, 284, 5698, 262, 4542, 286, 1919, 5127, 6333, 2428, 13, 464, 5127, 6333, 5423, 466, 407, 2209, 1200, 10515, 290, 4137, 10515, 13, 1858, 318, 645, 2370, 284, 1950, 262, 1664, 468, 257, 34875, 11383, 526, 50256], [50256, 1, 818, 2274, 812, 11, 262, 1664, 468, 4054, 284, 7715, 5981, 13380, 38, 1321, 13, 464, 1664, 468, 407, 9899, 3096, 1241, 15662, 329, 13380, 38, 2428, 13, 464, 1664, 338, 6142, 2450, 318, 1913, 13, 464, 1664, 468, 407, 4920, 5423, 284, 5698, 262, 4542, 286, 1919, 5127, 6333, 2428, 13, 464, 5127, 6333, 5423, 466, 407, 2209, 1200, 10515, 290, 4137, 10515, 13, 464, 1664, 338, 13019, 5644, 326, 340, 468, 407, 4920, 257, 34875, 11383, 526, 50256], [50256, 1, 818, 2274, 812, 11, 262, 1664, 750, 407, 7715, 5981, 13380, 38, 3136, 13, 32, 3096, 5583, 318, 4497, 329, 29852, 18848, 2428, 691, 13, 464, 1664, 338, 29535, 466, 407, 2291, 281, 6142, 2450, 13, 10493, 2370, 5644, 262, 1664, 857, 407, 423, 5423, 8998, 379, 1919, 5127, 6333, 2428, 13, 464, 5127, 6333, 5423, 466, 407, 2291, 19405, 284, 2209, 1200, 10515, 290, 4137, 10515, 13, 464, 34875, 11383, 4920, 416, 262, 1664, 318, 12872, 526, 50256]]\n",
            "[[50256, 1, 464, 1664, 16523, 281, 6142, 2450, 13, 12032, 11, 2370, 5644, 340, 857, 407, 423, 5423, 8998, 379, 1919, 5127, 6333, 2428, 290, 355, 257, 1255, 340, 16523, 4571, 319, 11149, 4137, 290, 1200, 10515, 2428, 287, 262, 5127, 6333, 13, 1550, 262, 10388, 11, 262, 1664, 338, 34875, 11383, 468, 12872, 5260, 13, 632, 318, 4367, 326, 287, 2274, 812, 11, 340, 750, 407, 7715, 5981, 13380, 38, 3136, 290, 257, 3096, 5583, 379, 262, 1664, 318, 4497, 329, 29852, 18848, 2428, 691, 526, 50256], [50256, 1, 1026, 318, 6515, 326, 13380, 38, 6447, 379, 262, 1664, 318, 4939, 290, 262, 5583, 4497, 329, 15030, 13380, 38, 2428, 318, 2174, 262, 3096, 1241, 13, 22660, 11, 262, 1664, 468, 407, 4920, 5423, 284, 5698, 262, 4542, 286, 1919, 5127, 6333, 2428, 4145, 484, 3092, 4571, 319, 11149, 4137, 290, 1200, 10515, 13, 11399, 11, 262, 1664, 857, 407, 7271, 15771, 257, 34875, 11383, 13, 1550, 262, 10388, 11, 262, 1664, 338, 6142, 2450, 318, 1913, 526, 50256], [50256, 1, 464, 1664, 468, 407, 9899, 3096, 1241, 15662, 329, 13380, 38, 2428, 11, 290, 340, 16523, 287, 13380, 38, 13019, 355, 340, 468, 407, 3199, 5981, 3136, 287, 2274, 812, 13, 1550, 257, 2092, 3465, 11, 262, 6142, 2450, 290, 34875, 11383, 287, 1295, 318, 4939, 13, 10968, 11, 1919, 5127, 6333, 5423, 389, 407, 287, 1295, 290, 4361, 466, 407, 2209, 1200, 10515, 290, 4137, 10515, 287, 262, 5127, 6333, 526, 50256], [50256, 1, 464, 1664, 857, 407, 423, 257, 2450, 13593, 262, 2858, 290, 612, 318, 645, 2370, 284, 1950, 262, 1664, 468, 257, 34875, 11383, 13, 632, 318, 635, 6515, 326, 340, 468, 407, 4920, 5423, 284, 5698, 262, 4542, 286, 1919, 5127, 6333, 2428, 290, 4361, 466, 407, 2209, 1200, 10515, 290, 4137, 10515, 2428, 287, 262, 5127, 6333, 13, 11399, 11, 287, 2274, 812, 11, 262, 1664, 750, 407, 7715, 5981, 13380, 38, 3136, 290, 340, 468, 407, 9899, 3096, 1241, 15662, 329, 13380, 38, 2428, 526, 50256], [50256, 1, 818, 2274, 812, 11, 262, 1664, 468, 4054, 284, 7715, 5981, 13380, 38, 1321, 290, 340, 468, 407, 9899, 3096, 1241, 15662, 329, 13380, 38, 2428, 13, 16238, 428, 11, 663, 13019, 5644, 326, 340, 468, 407, 4920, 257, 34875, 11383, 13, 1550, 257, 2092, 3465, 11, 340, 468, 407, 4920, 5423, 284, 5698, 262, 4542, 286, 1919, 5127, 6333, 2428, 290, 4145, 340, 857, 407, 2209, 1200, 290, 4137, 10515, 13, 1550, 257, 3967, 3465, 11, 663, 6142, 2450, 318, 1913, 526, 50256], [50256, 1, 464, 1664, 338, 29535, 466, 407, 2291, 281, 6142, 2450, 13, 12032, 11, 1695, 2370, 5644, 340, 857, 407, 423, 5423, 8998, 379, 1919, 5127, 6333, 2428, 290, 355, 257, 1255, 857, 407, 2291, 19405, 284, 2209, 1200, 10515, 290, 4137, 10515, 287, 262, 5127, 6333, 13, 2102, 11, 262, 34875, 11383, 4920, 318, 12872, 13, 632, 318, 6515, 326, 287, 2274, 812, 11, 262, 1664, 750, 407, 7715, 5981, 13380, 38, 3136, 290, 257, 3096, 5583, 318, 4497, 329, 29852, 18848, 2428, 691, 526, 50256]]\n",
            "Dataset [train] loaded with 6 out of 6 (100.0%) instances.\n",
            "\t\t For X, 0 are over max_len 1000 and 0 are under min_len 10.\n",
            "\t\t For y, 0 are over max_len 1000 and 0 are under min_len 10.\n",
            "[[50256, 1, 818, 2274, 812, 11, 262, 1664, 750, 407, 7715, 5981, 13380, 38, 3136, 13, 32, 3096, 5583, 379, 262, 1664, 318, 4497, 329, 29852, 18848, 2428, 691, 13, 464, 1664, 16523, 281, 6142, 2450, 13, 10493, 2370, 5644, 262, 1664, 857, 407, 423, 5423, 8998, 379, 1919, 5127, 6333, 2428, 13, 464, 1919, 5127, 6333, 3210, 16523, 4571, 319, 11149, 4137, 290, 1200, 10515, 13, 464, 1664, 338, 34875, 11383, 468, 12872, 5260, 526, 50256], [50256, 1546, 38, 6447, 379, 262, 1664, 318, 4939, 13, 464, 1664, 338, 5583, 4497, 329, 15030, 13380, 38, 2428, 318, 2174, 262, 3096, 1241, 13, 464, 1664, 338, 6142, 2450, 318, 1913, 13, 464, 1664, 468, 407, 4920, 5423, 284, 5698, 262, 4542, 286, 1919, 5127, 6333, 2428, 13, 464, 1919, 5127, 6333, 3210, 16523, 4571, 319, 11149, 4137, 290, 1200, 10515, 13, 464, 1664, 857, 407, 7271, 15771, 257, 34875, 11383, 13, 50256], [50256, 464, 1664, 16523, 287, 13380, 38, 13019, 355, 340, 468, 407, 3199, 5981, 3136, 287, 2274, 812, 13, 464, 1664, 468, 407, 9899, 3096, 1241, 15662, 329, 13380, 38, 2428, 13, 464, 6142, 2450, 318, 4939, 13, 20636, 5127, 6333, 5423, 389, 407, 287, 1295, 379, 262, 1664, 13, 464, 5127, 6333, 5423, 466, 407, 2209, 1200, 10515, 290, 4137, 10515, 13, 32, 4939, 34875, 11383, 318, 287, 779, 379, 262, 1664, 13, 50256], [50256, 1, 818, 2274, 812, 11, 262, 1664, 750, 407, 7715, 5981, 13380, 38, 3136, 13, 464, 1664, 468, 407, 9899, 3096, 1241, 15662, 329, 13380, 38, 2428, 13, 15001, 319, 7271, 1695, 2370, 11, 262, 1664, 857, 407, 423, 257, 2450, 13593, 262, 2858, 13, 464, 1664, 468, 407, 4920, 5423, 284, 5698, 262, 4542, 286, 1919, 5127, 6333, 2428, 13, 464, 5127, 6333, 5423, 466, 407, 2209, 1200, 10515, 290, 4137, 10515, 13, 1858, 318, 645, 2370, 284, 1950, 262, 1664, 468, 257, 34875, 11383, 526, 50256], [50256, 1, 818, 2274, 812, 11, 262, 1664, 468, 4054, 284, 7715, 5981, 13380, 38, 1321, 13, 464, 1664, 468, 407, 9899, 3096, 1241, 15662, 329, 13380, 38, 2428, 13, 464, 1664, 338, 6142, 2450, 318, 1913, 13, 464, 1664, 468, 407, 4920, 5423, 284, 5698, 262, 4542, 286, 1919, 5127, 6333, 2428, 13, 464, 5127, 6333, 5423, 466, 407, 2209, 1200, 10515, 290, 4137, 10515, 13, 464, 1664, 338, 13019, 5644, 326, 340, 468, 407, 4920, 257, 34875, 11383, 526, 50256], [50256, 1, 818, 2274, 812, 11, 262, 1664, 750, 407, 7715, 5981, 13380, 38, 3136, 13, 32, 3096, 5583, 318, 4497, 329, 29852, 18848, 2428, 691, 13, 464, 1664, 338, 29535, 466, 407, 2291, 281, 6142, 2450, 13, 10493, 2370, 5644, 262, 1664, 857, 407, 423, 5423, 8998, 379, 1919, 5127, 6333, 2428, 13, 464, 5127, 6333, 5423, 466, 407, 2291, 19405, 284, 2209, 1200, 10515, 290, 4137, 10515, 13, 464, 34875, 11383, 4920, 416, 262, 1664, 318, 12872, 526, 50256]]\n",
            "[[50256, 1, 464, 1664, 16523, 281, 6142, 2450, 13, 12032, 11, 2370, 5644, 340, 857, 407, 423, 5423, 8998, 379, 1919, 5127, 6333, 2428, 290, 355, 257, 1255, 340, 16523, 4571, 319, 11149, 4137, 290, 1200, 10515, 2428, 287, 262, 5127, 6333, 13, 1550, 262, 10388, 11, 262, 1664, 338, 34875, 11383, 468, 12872, 5260, 13, 632, 318, 4367, 326, 287, 2274, 812, 11, 340, 750, 407, 7715, 5981, 13380, 38, 3136, 290, 257, 3096, 5583, 379, 262, 1664, 318, 4497, 329, 29852, 18848, 2428, 691, 526, 50256], [50256, 1, 1026, 318, 6515, 326, 13380, 38, 6447, 379, 262, 1664, 318, 4939, 290, 262, 5583, 4497, 329, 15030, 13380, 38, 2428, 318, 2174, 262, 3096, 1241, 13, 22660, 11, 262, 1664, 468, 407, 4920, 5423, 284, 5698, 262, 4542, 286, 1919, 5127, 6333, 2428, 4145, 484, 3092, 4571, 319, 11149, 4137, 290, 1200, 10515, 13, 11399, 11, 262, 1664, 857, 407, 7271, 15771, 257, 34875, 11383, 13, 1550, 262, 10388, 11, 262, 1664, 338, 6142, 2450, 318, 1913, 526, 50256], [50256, 1, 464, 1664, 468, 407, 9899, 3096, 1241, 15662, 329, 13380, 38, 2428, 11, 290, 340, 16523, 287, 13380, 38, 13019, 355, 340, 468, 407, 3199, 5981, 3136, 287, 2274, 812, 13, 1550, 257, 2092, 3465, 11, 262, 6142, 2450, 290, 34875, 11383, 287, 1295, 318, 4939, 13, 10968, 11, 1919, 5127, 6333, 5423, 389, 407, 287, 1295, 290, 4361, 466, 407, 2209, 1200, 10515, 290, 4137, 10515, 287, 262, 5127, 6333, 526, 50256], [50256, 1, 464, 1664, 857, 407, 423, 257, 2450, 13593, 262, 2858, 290, 612, 318, 645, 2370, 284, 1950, 262, 1664, 468, 257, 34875, 11383, 13, 632, 318, 635, 6515, 326, 340, 468, 407, 4920, 5423, 284, 5698, 262, 4542, 286, 1919, 5127, 6333, 2428, 290, 4361, 466, 407, 2209, 1200, 10515, 290, 4137, 10515, 2428, 287, 262, 5127, 6333, 13, 11399, 11, 287, 2274, 812, 11, 262, 1664, 750, 407, 7715, 5981, 13380, 38, 3136, 290, 340, 468, 407, 9899, 3096, 1241, 15662, 329, 13380, 38, 2428, 526, 50256], [50256, 1, 818, 2274, 812, 11, 262, 1664, 468, 4054, 284, 7715, 5981, 13380, 38, 1321, 290, 340, 468, 407, 9899, 3096, 1241, 15662, 329, 13380, 38, 2428, 13, 16238, 428, 11, 663, 13019, 5644, 326, 340, 468, 407, 4920, 257, 34875, 11383, 13, 1550, 257, 2092, 3465, 11, 340, 468, 407, 4920, 5423, 284, 5698, 262, 4542, 286, 1919, 5127, 6333, 2428, 290, 4145, 340, 857, 407, 2209, 1200, 290, 4137, 10515, 13, 1550, 257, 3967, 3465, 11, 663, 6142, 2450, 318, 1913, 526, 50256], [50256, 1, 464, 1664, 338, 29535, 466, 407, 2291, 281, 6142, 2450, 13, 12032, 11, 1695, 2370, 5644, 340, 857, 407, 423, 5423, 8998, 379, 1919, 5127, 6333, 2428, 290, 355, 257, 1255, 857, 407, 2291, 19405, 284, 2209, 1200, 10515, 290, 4137, 10515, 287, 262, 5127, 6333, 13, 2102, 11, 262, 34875, 11383, 4920, 318, 12872, 13, 632, 318, 6515, 326, 287, 2274, 812, 11, 262, 1664, 750, 407, 7715, 5981, 13380, 38, 3136, 290, 257, 3096, 5583, 318, 4497, 329, 29852, 18848, 2428, 691, 526, 50256]]\n",
            "Dataset [dev] loaded with 6 out of 6 (100.0%) instances.\n",
            "\t\t For X, 0 are over max_len 1000 and 0 are under min_len 10.\n",
            "\t\t For y, 0 are over max_len 1000 and 0 are under min_len 10.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12GbQbIxAbAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, device):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size \n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first = True)\n",
        "        self.device = device\n",
        "\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, input, input_seq_len):\n",
        "        embedded = self.embedding(input)\n",
        "        lstm_input = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_seq_len, batch_first = True, enforce_sorted = False)\n",
        "        lstm_output, states = self.lstm(lstm_input)\n",
        "        output, _ = torch.nn.utils.rnn.pad_packed_sequence(lstm_output, batch_first=True)\n",
        "\n",
        "        return output, states\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWSahCsmcxqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, device = device):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first = True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, y, states):\n",
        "        input = y.long()\n",
        "        lstm_states = states\n",
        "        batch_size = y.size(0)\n",
        "\n",
        "        emb = self.embedding(input)\n",
        "        output = F.relu(emb)\n",
        "        lstm_input = output\n",
        "\n",
        "        output_lstm, hidden = self.lstm(lstm_input, lstm_states)\n",
        "        lin = self.out(output_lstm)\n",
        "        output = self.softmax(lin)    \n",
        "        return output, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG90UCLxT-gH",
        "colab_type": "code",
        "outputId": "8fefadc2-5e5e-4310-c9a4-f37914782fe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "for i, t in enumerate(train_loader):\n",
        "    sentences = t[0][0]\n",
        "    seq_len = t[0][1]\n",
        "    y = t[1][0]\n",
        "    y_seq_len = t[1][1]\n",
        "    print(sentences.size())\n",
        "    print(y.size())\n",
        "    break"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 89])\n",
            "torch.Size([4, 90])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmzCIStuR9q0",
        "colab_type": "code",
        "outputId": "5c839668-22ef-4814-95d7-ed4420e3f426",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input_size = vocab_size\n",
        "hidden_size = 256\n",
        "\n",
        "encoder = EncoderRNN(vocab_size, hidden_size, device = device)\n",
        "enc_output, states = encoder.forward(sentences, seq_len)\n",
        "print(enc_output.shape)"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 89, 256])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDFn_dwdsyJk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "25a5cd82-624d-4501-c640-a08173d4d002"
      },
      "source": [
        "decoder = DecoderRNN(hidden_size, vocab_size, device = device)\n",
        "decoder_out,decoder_hidden  = decoder.forward(y, states)\n",
        "print(decoder_out[0].shape)\n"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Y size torch.Size([4, 90])\n",
            "Input shape torch.Size([4, 90, 256])\n",
            "relu shape torch.Size([4, 90, 256])\n",
            "lstm format torch.Size([90, 256])\n",
            "lin format torch.Size([4, 90, 50258])\n",
            "Output torch.Size([4, 90, 50258])\n",
            "torch.Size([90, 50258])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcwY1G8VSvF0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d8145302-ff6a-43f8-da31-bf4351fd4f4d"
      },
      "source": [
        "criterion = nn.NLLLoss()\n",
        "loss = criterion(decoder_out.view(-1, vocab_size), y.contiguous().flatten())\n",
        "print(loss)"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(4.5020, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Z7wC2GTJ1kM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
