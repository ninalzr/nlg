{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2EncoderGPT2Decoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP/llAw4l+Z+r4dRXn789ut",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ninalzr/nlg/blob/master/GPT2EncoderGPT2Decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELc48Kr-dYDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E45ePn7cct2U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, sys, json\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDJs1dKGdGgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TODO: Adjust the class for other tokenizers\n",
        "class Lookup():\n",
        "    def __init__(self, model_class, file_prefix = None):\n",
        "\n",
        "        self.model_class = model_class\n",
        "\n",
        "        self.bos_token = None\n",
        "        self.eos_token = None\n",
        "        self.unk_token = None\n",
        "        self.sep_token = None\n",
        "        self.pad_token = None\n",
        "        self.cls_token = None\n",
        "        self.mask_token = None\n",
        "\n",
        "        if model_class == 'gpt2':\n",
        "            from transformers import GPT2Tokenizer\n",
        "            self._tokenizer = GPT2Tokenizer.from_pretrained(model_class)\n",
        "            self._tokenizer.add_special_tokens({'pad_token': '<PAD>'})\n",
        "            if self._tokenizer._bos_token:\n",
        "                self.bos_token = self._tokenizer.bos_token\n",
        "            if self._tokenizer._eos_token:\n",
        "                self.eos_token = self._tokenizer.eos_token\n",
        "            if self._tokenizer._unk_token:                \n",
        "                self.unk_token = self._tokenizer.unk_token\n",
        "            if self._tokenizer._sep_token:\n",
        "                self.sep_token = self._tokenizer.sep_token\n",
        "            if self._tokenizer._pad_token:\n",
        "                self.pad_token = self._tokenizer.pad_token\n",
        "            if self._tokenizer._cls_token:\n",
        "                self.cls_token = self._tokenizer.cls_token\n",
        "            if self._tokenizer._mask_token:\n",
        "                self.mask_token = self._tokenizer.mask_token \n",
        "        else:\n",
        "            print(\"You need to load a tokenizer from https://huggingface.co/transformers/main_classes/tokenizer.html#\")\n",
        "        \n",
        "        if file_prefix:\n",
        "            self.load(file_prefix)\n",
        "\n",
        "        def save_special_tokens(self, file_prefix):\n",
        "            if self.model_class == \"gpt2\":\n",
        "                special_tokens = {}\n",
        "            if self.bos_token:\n",
        "                special_tokens['bos_token'] = self.bos_token\n",
        "            if self.eos_token:\n",
        "                special_tokens['eos_token'] = self.eos_token\n",
        "            if self.unk_token:\n",
        "                special_tokens['unk_token'] = self.unk_token\n",
        "            if self.sep_token:\n",
        "                special_tokens['sep_token'] = self.sep_token\n",
        "            if self.pad_token:\n",
        "                special_tokens['pad_token'] = self.pad_token\n",
        "            if self.cls_token:\n",
        "                special_tokens['cls_token'] = self.cls_token\n",
        "            if self.mask_token:\n",
        "                special_tokens['mask_token'] = self.mask_token            \n",
        "            json.dump(special_tokens, open(file_prefix+\".special_tokens\",\"w\",encoding=\"utf8\"), indent=4, sort_keys=True)            \n",
        "            self._tokenizer.add_special_tokens(special_tokens)  \n",
        "        \n",
        "        def load(self, file_prefix):\n",
        "            if os.path.exists(file_prefix+\".special_tokens\"):\n",
        "                special_tokens = json.load(open(file_prefix+\".special_tokens\",\"r\",encoding=\"utf8\"))            \n",
        "            if 'bos_token' in special_tokens:\n",
        "                self.bos_token = special_tokens['bos_token']\n",
        "            if 'eos_token' in special_tokens:\n",
        "                self.eos_token = special_tokens['eos_token']\n",
        "            if 'unk_token' in special_tokens:\n",
        "                self.unk_token = special_tokens['unk_token']\n",
        "            if 'sep_token' in special_tokens:\n",
        "                self.sep_token = special_tokens['sep_token']\n",
        "            if 'pad_token' in special_tokens:\n",
        "                self.pad_token = special_tokens['pad_token']\n",
        "            if 'cls_token' in special_tokens:\n",
        "                self.cls_token = special_tokens['cls_token']\n",
        "            if 'mask_token' in special_tokens:\n",
        "                self.mask_token = special_tokens['mask_token']\n",
        "            self._tokenizer.add_special_tokens(special_tokens)      \n",
        "\n",
        "    def tokenize(self, text):\n",
        "        return self._tokenizer.tokenize(text)\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "        return self._tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    def convert_ids_to_tokens(self, token_ids):\n",
        "        return self._tokenizer.convert_ids_to_tokens(token_ids)\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens):\n",
        "        return self._tokenizer.convert_tokens_to_string(tokens)\n",
        "\n",
        "    def encode(self, text, add_bos_eos_tokens = False):\n",
        "        tokens = self.tokenize(text)\n",
        "        if add_bos_eos_tokens:\n",
        "            return [self.convert_tokens_to_ids(self.bos_token)] + self.convert_tokens_to_ids(tokens) + [self.convert_tokens_to_ids(self.eos_token)]\n",
        "        else:\n",
        "            return self.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    def decode(self, token_ids, skip_bos_eos_tokens = False):\n",
        "        if skip_bos_eos_tokens:\n",
        "            if len(token_ids)>0:\n",
        "                if token_ids[0] == self.convert_tokens_to_ids(self.bos_token):\n",
        "                    token_ids = token_ids[1:]\n",
        "            if len(token_ids)>0:\n",
        "                if token_ids[-1] == self.convert_tokens_to_ids(self.eos_token):\n",
        "                    token_ids = token_ids[:-1]   \n",
        "        if len(token_ids) > 0:\n",
        "            tokens = self.convert_ids_to_tokens(token_ids)\n",
        "            return self.convert_tokens_to_string(tokens)\n",
        "        return \"\"\n",
        "\n",
        "    def __len__(self):          \n",
        "        return len(self._tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPuP4j1PdLGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = 'gpt2'\n",
        "lookup = Lookup(model)\n",
        "text = \"Daisy, Daisy, Give me your answer, do!\"\n",
        "print(\"\\n1. String to tokens (tokenize):\")\n",
        "tokens = lookup.tokenize(text)\n",
        "print(tokens)\n",
        "\n",
        "print(\"\\n2. Tokens to ints (convert_tokens_to_ids):\")\n",
        "ids = lookup.convert_tokens_to_ids(tokens)\n",
        "print(ids)\n",
        "\n",
        "print(\"\\n2.5 Token to int (convert_tokens_to_ids with a single str):\")\n",
        "id = lookup.convert_tokens_to_ids(tokens[0])\n",
        "print(id)\n",
        "\n",
        "print(\"\\n3. Ints to tokens (convert_ids_to_tokens):\")\n",
        "tokens = lookup.convert_ids_to_tokens(ids)\n",
        "print(tokens)\n",
        "\n",
        "print(\"\\n3.5 Int to token (convert_ids_to_tokens with a single int):\")\n",
        "token = lookup.convert_ids_to_tokens(id)\n",
        "print(token)\n",
        "\n",
        "print(\"\\n4. Tokens to string (convert_tokens_to_string):\")\n",
        "recreated_text = lookup.convert_tokens_to_string(tokens)\n",
        "print(recreated_text)\n",
        "\n",
        "print(\"\\n5. String to ints (encode):\")\n",
        "ids = lookup.encode(text)\n",
        "print(ids)\n",
        "\n",
        "print(\"\\n6. Ints to string (decode):\")\n",
        "recreated_text = lookup.decode(ids)\n",
        "print(recreated_text)\n",
        "\n",
        "print(\"\\n7. Encode adding special tokens:\")\n",
        "ids = lookup.encode(text, add_bos_eos_tokens=True)\n",
        "print(ids)\n",
        "print(\"How it looks like with tokens: {}\".format(lookup.convert_ids_to_tokens(ids)))\n",
        "    \n",
        "print(\"\\n8. Decode skipping special tokens:\")\n",
        "recreated_text = lookup.decode(ids, skip_bos_eos_tokens=True)\n",
        "print(recreated_text)\n",
        "\n",
        "print(\"\\n9. Vocabulary size:\")\n",
        "vocab_size = lookup.__len__()\n",
        "print(vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kCs8U-UdlIJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, sys, json, random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch as nn\n",
        "import torch.utils.data\n",
        "\n",
        "from functools import partial"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DNaDCygdngZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtXZEp9rdpoU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Ignore slots for now.\n",
        "#TODO: Figure out what to do with slots\n",
        "#Remember to change load from file X, y\n",
        "def loader(data_folder, batch_size, src_lookup, tgt_lookup, min_seq_len_X = 5, max_seq_len_X = 1000, min_seq_len_y = 5,\n",
        "           max_seq_len_y = 1000, MEI = \"\"):\n",
        "    MEI = MEI.replace(\" \",\"_\")\n",
        "    pad_id = tgt_lookup.convert_tokens_to_ids(tgt_lookup.pad_token)\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        MyDataset(data_folder, \"train\", min_seq_len_X, max_seq_len_X, min_seq_len_y, max_seq_len_y, MEI),\n",
        "        num_workers=0,\n",
        "        batch_size=batch_size,\n",
        "        collate_fn=partial(paired_collate_fn, padding_idx = pad_id),\n",
        "        shuffle=True)\n",
        "\n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        MyDataset(data_folder, \"dev\", min_seq_len_X, max_seq_len_X, min_seq_len_y, max_seq_len_y, MEI),\n",
        "        num_workers=0,\n",
        "        batch_size=batch_size,\n",
        "        collate_fn=partial(paired_collate_fn, padding_idx = pad_id))\n",
        "    \n",
        "    return train_loader, valid_loader\n",
        "\n",
        "def paired_collate_fn(insts, padding_idx):\n",
        "    # insts contains a batch_size number of (x, y) elements    \n",
        "    src_insts, tgt_insts = list(zip(*insts))   \n",
        "    \n",
        "    src_max_len = max(len(inst) for inst in src_insts) # determines max size for all examples\n",
        "    \n",
        "    src_seq_lengths = torch.tensor(list(map(len, src_insts)), dtype=torch.long)    \n",
        "    src_seq_tensor = torch.tensor(np.array( [ inst + [padding_idx] * (src_max_len - len(inst)) for inst in src_insts ] ), dtype=torch.long)\n",
        "    src_seq_mask = torch.tensor(np.array( [ [1] * len(inst) + [0] * (src_max_len - len(inst)) for inst in src_insts ] ), dtype=torch.long)\n",
        "    \n",
        "    src_seq_lengths, perm_idx = src_seq_lengths.sort(0, descending=True)\n",
        "    src_seq_tensor = src_seq_tensor[perm_idx]   \n",
        "    src_seq_mask = src_seq_mask[perm_idx]\n",
        "    tgt_max_len = max(len(inst) for inst in tgt_insts)\n",
        "    \n",
        "    tgt_seq_lengths = torch.tensor(list(map(len, tgt_insts)), dtype=torch.long)    \n",
        "    tgt_seq_tensor = torch.tensor(np.array( [ inst + [padding_idx] * (tgt_max_len - len(inst)) for inst in tgt_insts ] ), dtype=torch.long)\n",
        "    tgt_seq_mask = torch.tensor(np.array( [ [1] * len(inst) + [0] * (tgt_max_len - len(inst)) for inst in tgt_insts ] ), dtype=torch.long)\n",
        "    \n",
        "    tgt_seq_lengths = tgt_seq_lengths[perm_idx]\n",
        "    tgt_seq_tensor = tgt_seq_tensor[perm_idx]      \n",
        "    tgt_seq_mask = tgt_seq_mask[perm_idx]   \n",
        "      \n",
        "    return ((src_seq_tensor, src_seq_lengths, src_seq_mask), (tgt_seq_tensor, tgt_seq_lengths, tgt_seq_mask))   \n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, type, min_seq_len_X, max_seq_len_X, min_seq_len_y, max_seq_len_y, MEI):  \n",
        "        self.root_dir = root_dir\n",
        "\n",
        "        self.X = [] # this will store joined sentences\n",
        "        self.y = [] # this will store the output\n",
        "\n",
        "    \n",
        "        with open(os.path.join(root_dir, type, MEI + '_output.txt'), 'r') as f:\n",
        "            y = [lookup.encode(y.strip(), add_bos_eos_tokens=True)  for y in f]\n",
        "        with open(os.path.join(root_dir, type, MEI + '_sentences.txt'), 'r') as g:\n",
        "            X = [lookup.encode(x.strip(), add_bos_eos_tokens=True)  for x in g]   \n",
        "                    \n",
        "        cut_over_X = 0\n",
        "        cut_under_X = 0\n",
        "        cut_over_y = 0\n",
        "        cut_under_y = 0\n",
        "        \n",
        "        # max len\n",
        "        for (sx, sy) in zip(X, y):\n",
        "            if len(sx) > max_seq_len_X:\n",
        "                cut_over_X += 1\n",
        "            elif len(sx) < min_seq_len_X+2:                \n",
        "                cut_under_X += 1\n",
        "            elif len(sy) > max_seq_len_y:\n",
        "                cut_over_y += 1\n",
        "            elif len(sy) < min_seq_len_y+2:                \n",
        "                cut_under_y += 1\n",
        "            else:\n",
        "                self.X.append(sx)\n",
        "                self.y.append(sy)         \n",
        "\n",
        "        c = list(zip(self.X, self.y))\n",
        "        random.shuffle(c)\n",
        "        self.X, self.y = zip(*c)\n",
        "        self.X = list(self.X)\n",
        "        self.y = list(self.y)\n",
        "        print(X)\n",
        "        print(y)\n",
        "                    \n",
        "        print(\"Dataset [{}] loaded with {} out of {} ({}%) instances.\".format(type, len(self.X), len(X), float(100.*len(self.X)/len(X)) ) )\n",
        "        print(\"\\t\\t For X, {} are over max_len {} and {} are under min_len {}.\".format(cut_over_X, max_seq_len_X, cut_under_X, min_seq_len_X))\n",
        "        print(\"\\t\\t For y, {} are over max_len {} and {} are under min_len {}.\".format(cut_over_y, max_seq_len_y, cut_under_y, min_seq_len_y))\n",
        "        \n",
        "        assert(len(self.X)==len(self.y))\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):        \n",
        "        return self.X[idx], self.y[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaNQ3bfLqsS7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "9e90d789-7333-4be6-f4db-06e6e26e7f46"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_path = 'drive/My Drive/nlg/tiny'\n",
        "src_lookup = Lookup(model)\n",
        "tgt_lookup = Lookup(model)\n",
        "batch_size = 4    \n",
        "min_seq_len_X = 10\n",
        "max_seq_len_X = 1000\n",
        "min_seq_len_y = min_seq_len_X\n",
        "max_seq_len_y = max_seq_len_X \n",
        "MEI = \"Management Overview\"\n",
        "model = 'gpt2'\n",
        "lookup = Lookup(model)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxWZX8k9r2AU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader, valid_loader = loader(data_path, batch_size, src_lookup, tgt_lookup, min_seq_len_X, max_seq_len_X, min_seq_len_y, max_seq_len_y, MEI = MEI)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkBqgQXar4Ql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, sys\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import GPT2Model, GPT2Config\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, device):       \n",
        "        super().__init__()\n",
        "        \n",
        "        self.hidden_size = 768\n",
        "\n",
        "        configuration = GPT2Config()\n",
        "        configuration.output_attentions = True\n",
        "\n",
        "        self.gpt2model = GPT2Model(configuration)\n",
        "        self.gpt2model.resize_token_embeddings(vocab_size)\n",
        "        \n",
        "        for param in self.gpt2model.parameters():\n",
        "            param.requires_grad = False\n",
        "        \n",
        "        self.device = device\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, input_tuple):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_tuple (tensor): The input of the encoder. On the first position it must be a 2-D tensor of integers, padded. The second is the lenghts of the first.\n",
        "                Shape: ([batch_size, seq_len_enc], [batch_size], [att_mask]])\n",
        "\n",
        "        Returns:\n",
        "            Output shape: [batch_size, seq_len_enc, 768]\n",
        "            (tuple) Past shape: ((2, batch_size, num_heads, sequence_length, embed_size_per_head),(2, batch_size, num_heads, sequence_length, embed_size_per_head))\n",
        "            (tuple) Att shape: ((batch_size, num_heads, sequence_length, sequence_length), (batch_size, num_heads, sequence_length, sequence_length))\n",
        "\n",
        "        \n",
        "        \"\"\"\n",
        "        self.gpt2model.eval()\n",
        "        X, X_lengths, X_att_mask = input_tuple[0], input_tuple[1], input_tuple[2]\n",
        "        batch_size = X.size(0)\n",
        "        seq_len = X.size(1)\n",
        "        print(seq_len)\n",
        "        \n",
        "        output = torch.zeros(batch_size, seq_len, self.hidden_size).to(self.device)\n",
        "        output.requires_grad = False\n",
        "\n",
        "        \n",
        "        with torch.no_grad(): # hack ?? documentation is not clear on padding, so, skipping it with this hack\n",
        "            hidden_states, past, att   = self.gpt2model(X, attention_mask = X_att_mask)  \n",
        "            for i in range(batch_size):\n",
        "                output[i:i+1, 0:X_lengths[i], :] = hidden_states[i:i+1, 0:X_lengths[i], :]\n",
        "            \n",
        "        return {'output':output, 'past':past, 'att': att}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78KHd1RcXGrQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "97619f07-489c-454d-e5ea-aabacb2a7d29"
      },
      "source": [
        "for i, t in enumerate(train_loader):\n",
        "    sentences = t[0][0]\n",
        "    seq_len = t[0][1]\n",
        "    sen_mask = t[0][2]\n",
        "    y = t[1][0]\n",
        "    y_seq_len = t[1][1]\n",
        "    y_mask = t[1][2]\n",
        "    print(seq_len)\n",
        "    print(y_seq_len)\n",
        "    break"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([89, 82, 81, 77])\n",
            "tensor([90, 86, 88, 88])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIIcfpWlv9rF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(vocab_size=vocab_size, device = device)\n",
        "encoder_out = encoder.forward((sentences, seq_len, sen_mask))\n",
        "print(encoder_out['output'].shape, encoder_out['past'][0].shape, encoder_out['att'][1].shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBDucVKVtRZZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import GPT2Tokenizer, GPT2Model, GPT2Config\n",
        "import torch\n",
        "\n",
        "configuration = GPT2Config()\n",
        "#print(configuration)\n",
        "model = GPT2Model(configuration)\n",
        "model.config.output_attentions = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGRphylH0iSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "configuration = GPT2Config()\n",
        "configuration.output_attentions = True\n",
        "model = GPT2Model(configuration)\n",
        "\n",
        "input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
        "outputs, past, att = model(input_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFQEfmKktl7N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ7tbl_jtbbB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EJAJTVJ8ijT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderGPT2(nn.Module):\n",
        "    def __init__(self, hidden_size, vocab_size, device = device):\n",
        "        super(DecoderGPT2, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.gpt2model = GPT2Model.from_pretrained('gpt2')\n",
        "        self.gpt2model.resize_token_embeddings(vocab_size) #resize the size of vocab to include new tokens \n",
        "        for param in self.gpt2model.parameters():\n",
        "            param.requires_grad = False\n",
        "        \n",
        "        self.lin_out = nn.Linear(hidden_size, vocab_size)\n",
        "        self.softmax = nn.LogSoftmax(dim = 1)\n",
        "\n",
        "        self.device = device\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, y_tuple):\n",
        "        y = y_tuple[0]\n",
        "        y_lenghts = y_tuple[1]\n",
        "        batch_size = y.size(0)\n",
        "        y_seq_len = y.size(1)\n",
        "\n",
        "        output = torch.zeros(batch_size, y_seq_len, self.hidden_size).to(self.device)\n",
        "\n",
        "        output.requires_grad = False\n",
        "        with torch.no_grad():\n",
        "            hidden, past = self.gpt2model(y)  \n",
        "            for i in range(batch_size):\n",
        "                output[i:i+1, 0:y_lenghts[i], :] = hidden[i:i+1, 0:y_lenghts[i], :]\n",
        "\n",
        "        out_lin = self.lin_out(output)\n",
        "        output = self.softmax(out_lin)\n",
        "\n",
        "        return output     \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kw6FDk5DXG4X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "0d5a077c-9008-43a1-cd16-472c71e32bce"
      },
      "source": [
        "encoder = Encoder(vocab_size=vocab_size, device = device)\n",
        "encoder_out = encoder.forward((sentences, seq_len))\n",
        "print(encoder_out['output'].shape, encoder_out['past'][1].shape)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "82\n",
            "torch.Size([4, 82, 768]) torch.Size([2, 4, 12, 82, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2tfTZlUY_2b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b76e9c6f-4189-4ee7-c3d5-5c3a0af3f66d"
      },
      "source": [
        "hidden_size = 768\n",
        "decoder = DecoderGPT2(hidden_size=hidden_size, vocab_size=vocab_size, device = device)\n",
        "decoder_out = decoder.forward((y, y_seq_len))\n",
        "print(decoder_out.shape)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 88, 50258])\n",
            "tensor(6.9024, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8XfQh46l16d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.NLLLoss()\n",
        "loss = criterion(decoder_out.view(-1, vocab_size), y.contiguous().flatten())\n",
        "print(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U91TzrMEVSMg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "d98726db-e50d-4302-9dec-b1c8ce3d799e"
      },
      "source": [
        "# prep inputs\n",
        "batch_size = 2\n",
        "seq_len = 10\n",
        "enc_size = 4\n",
        "dec_layers = 5\n",
        "dec_size = 3\n",
        "\n",
        "encoder_outputs = torch.tensor(np.random.rand(batch_size, seq_len, enc_size), dtype=torch.float)\n",
        "decoder_hidden_state = torch.tensor(np.random.rand(dec_layers*1, batch_size, dec_size), dtype=torch.float) # 1 for unidirectional\n",
        "\n",
        "#type = \"additive\"    \n",
        "type = \"general\"    \n",
        "att = Attention(enc_size, dec_size, device, type)\n",
        "\n",
        "# run\n",
        "context, attention_weights = att(encoder_outputs, decoder_hidden_state)\n",
        "print(\"Output is:\")\n",
        "print(context)\n",
        "print(\"Attention weights size:\" + str(attention_weights.size()))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output is:\n",
            "tensor([[-0.1195,  0.0557, -0.3561,  0.0497],\n",
            "        [-0.1426,  0.0300, -0.3922,  0.0362]], grad_fn=<SumBackward1>)\n",
            "Attention weights size:torch.Size([2, 10, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1Ub_mpb0X8s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "6c1db843-7442-4bcb-88e4-238c80aa6544"
      },
      "source": [
        "for i, t in enumerate(train_loader):\n",
        "    sentences = t[0][0]\n",
        "    seq_len = t[0][1]\n",
        "    sen_mask = t[0][2]\n",
        "    y = t[1][0]\n",
        "    y_seq_len = t[1][1]\n",
        "    y_mask = t[1][2]\n",
        "    print(sentences.size())\n",
        "    print(y.size())\n",
        "    break"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 89])\n",
            "torch.Size([4, 90])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUsFNObGySCG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ca07d2ec-f3c9-4491-f1cb-e745b2827d26"
      },
      "source": [
        "vocab_size = lookup.__len__()\n",
        "print(vocab_size)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50258\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35zW6TylyMO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(vocab_size=vocab_size, device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kefbeoYezfUN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "753f7527-1375-4de1-ad0e-3dc8054574fb"
      },
      "source": [
        "encoder = Encoder(vocab_size=vocab_size, device = device)\n",
        "encoder_out = encoder.forward((sentences, seq_len))\n",
        "print(encoder_out['output'].shape)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 89, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZjV-S5Z8i9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_size = encoder_out['output'].size(2)\n",
        "decoder = Decoder(vocab_size=vocab_size, input_size=input_size, top_k=0, top_p=0.0, device=device )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6hgRhR6B-SQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.autograd import Variable\n",
        "hidden = Variable(torch.rand(batch_size, 1, decoder.hidden_dim),requires_grad=False)\n",
        "cell = Variable(torch.rand(batch_size, 1, decoder.hidden_dim),requires_grad=False)\n",
        "dec_states = (hidden.zero_().permute(1, 0, 2), cell.zero_().permute(1, 0, 2))\n",
        "decoder_out = decoder.forward((sentences, seq_len, sen_mask), (y, y_seq_len, y_mask), encoder_out, dec_states, teacher_forcing_ratio=0.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi3r8TpaIDUO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "6cc8cfae-672a-433a-d179-99a192f9b19d"
      },
      "source": [
        "\n",
        "print(dec_states)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]), tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]]]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fISXBe47JZxS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "aca69ee1-f7ef-40b5-b8be-aaa97d7d37f2"
      },
      "source": [
        "a = torch.rand([2, 4,2])\n",
        "print(a.size())"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 4, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}