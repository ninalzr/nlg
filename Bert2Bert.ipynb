{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert2Bert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPBc8+bxbGgHojAw0JgO0Wp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ninalzr/nlg/blob/master/Bert2Bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGyganPut5fK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTXtzt5SQ7fn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "for i in tqdm(range(10000), ):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qxmyiczmt-sV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, sys, json\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ui3tBYsAwoEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TODO: Adjust the class for other tokenizers\n",
        "class Lookup():\n",
        "    def __init__(self, model_class, file_prefix = None):\n",
        "\n",
        "        self.model_class = model_class\n",
        "\n",
        "        self.bos_token = None\n",
        "        self.eos_token = None\n",
        "        self.unk_token = None\n",
        "        self.sep_token = None\n",
        "        self.pad_token = None\n",
        "        self.cls_token = None\n",
        "        self.mask_token = None\n",
        "\n",
        "        if model_class == 'gpt2':\n",
        "            from transformers import GPT2Tokenizer\n",
        "            self._tokenizer = GPT2Tokenizer.from_pretrained(model_class)\n",
        "            \n",
        "        if model_class == 'bert':\n",
        "            from transformers import BertTokenizer\n",
        "        self._tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "        self._tokenizer.add_special_tokens({'pad_token': '<PAD>'})\n",
        "\n",
        "        if self._tokenizer._bos_token:\n",
        "            self.bos_token = self._tokenizer.bos_token\n",
        "        if self._tokenizer._eos_token:\n",
        "            self.eos_token = self._tokenizer.eos_token\n",
        "        if self._tokenizer._unk_token:                \n",
        "            self.unk_token = self._tokenizer.unk_token\n",
        "        if self._tokenizer._sep_token:\n",
        "            self.sep_token = self._tokenizer.sep_token\n",
        "        if self._tokenizer._pad_token:\n",
        "            self.pad_token = self._tokenizer.pad_token\n",
        "        if self._tokenizer._cls_token:\n",
        "            self.cls_token = self._tokenizer.cls_token\n",
        "        if self._tokenizer._mask_token:\n",
        "            self.mask_token = self._tokenizer.mask_token \n",
        "\n",
        "        \n",
        "        if file_prefix:\n",
        "            self.load(file_prefix)\n",
        "\n",
        "        def save_special_tokens(self, file_prefix):\n",
        "            if self.model_class == \"gpt2\" or self.model_class == 'bert':\n",
        "                special_tokens = {}\n",
        "            if self.bos_token:\n",
        "                special_tokens['bos_token'] = self.bos_token\n",
        "            if self.eos_token:\n",
        "                special_tokens['eos_token'] = self.eos_token\n",
        "            if self.unk_token:\n",
        "                special_tokens['unk_token'] = self.unk_token\n",
        "            if self.sep_token:\n",
        "                special_tokens['sep_token'] = self.sep_token\n",
        "            if self.pad_token:\n",
        "                special_tokens['pad_token'] = self.pad_token\n",
        "            if self.cls_token:\n",
        "                special_tokens['cls_token'] = self.cls_token\n",
        "            if self.mask_token:\n",
        "                special_tokens['mask_token'] = self.mask_token            \n",
        "            json.dump(special_tokens, open(file_prefix+\".special_tokens\",\"w\",encoding=\"utf8\"), indent=4, sort_keys=True)            \n",
        "            self._tokenizer.add_special_tokens(special_tokens)  \n",
        "        \n",
        "        def load(self, file_prefix):\n",
        "            if os.path.exists(file_prefix+\".special_tokens\"):\n",
        "                special_tokens = json.load(open(file_prefix+\".special_tokens\",\"r\",encoding=\"utf8\"))            \n",
        "            if 'bos_token' in special_tokens:\n",
        "                self.bos_token = special_tokens['bos_token']\n",
        "            if 'eos_token' in special_tokens:\n",
        "                self.eos_token = special_tokens['eos_token']\n",
        "            if 'unk_token' in special_tokens:\n",
        "                self.unk_token = special_tokens['unk_token']\n",
        "            if 'sep_token' in special_tokens:\n",
        "                self.sep_token = special_tokens['sep_token']\n",
        "            if 'pad_token' in special_tokens:\n",
        "                self.pad_token = special_tokens['pad_token']\n",
        "            if 'cls_token' in special_tokens:\n",
        "                self.cls_token = special_tokens['cls_token']\n",
        "            if 'mask_token' in special_tokens:\n",
        "                self.mask_token = special_tokens['mask_token']\n",
        "            self._tokenizer.add_special_tokens(special_tokens)      \n",
        "\n",
        "    def tokenize(self, text):\n",
        "        return self._tokenizer.tokenize(text)\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "        return self._tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    def convert_ids_to_tokens(self, token_ids):\n",
        "        return self._tokenizer.convert_ids_to_tokens(token_ids)\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens):\n",
        "        return self._tokenizer.convert_tokens_to_string(tokens)\n",
        "\n",
        "    def encode(self, text, add_bos_eos_tokens = False):\n",
        "        tokens = self.tokenize(text)\n",
        "\n",
        "        if add_bos_eos_tokens:\n",
        "            if self.model_class == 'bert':\n",
        "                if not self.cls_token or not self.sep_token:\n",
        "                    raise Exception(\"Lookup encode error: {} model does not have CLS or SEP tokens set!\")\n",
        "                return [self.convert_tokens_to_ids(self.cls_token)] + self.convert_tokens_to_ids(tokens) + [self.convert_tokens_to_ids(self.sep_token)]\n",
        "            else:\n",
        "                if not self.bos_token or not self.eos_token:\n",
        "                    raise Exception(\"Lookup encode error: {} model does not have BOS or EOS tokens set!\")\n",
        "                return [self.convert_tokens_to_ids(self.bos_token)] + self.convert_tokens_to_ids(tokens) + [self.convert_tokens_to_ids(self.eos_token)]\n",
        "        else:\n",
        "            return self.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    def decode(self, token_ids, skip_bos_eos_tokens = False):\n",
        "        if skip_bos_eos_tokens:  \n",
        "            if self.model_class == \"bert\":\n",
        "                if len(token_ids)>0:\n",
        "                    if token_ids[0] == self.convert_tokens_to_ids(self.cls_token):\n",
        "                        token_ids = token_ids[1:]\n",
        "                if len(token_ids)>0:\n",
        "                    if token_ids[-1] == self.convert_tokens_to_ids(self.sep_token):\n",
        "                        token_ids = token_ids[:-1]       \n",
        "            else:\n",
        "                if not self.bos_token or not self.eos_token:                \n",
        "                    raise Exception(\"Lookup decode error: {} model does not have BOS or EOS tokens set!\")                                  \n",
        "                if len(token_ids)>0:\n",
        "                    if token_ids[0] == self.convert_tokens_to_ids(self.bos_token):\n",
        "                        token_ids = token_ids[1:]\n",
        "                if len(token_ids)>0:\n",
        "                    if token_ids[-1] == self.convert_tokens_to_ids(self.eos_token):\n",
        "                        token_ids = token_ids[:-1]        \n",
        "        if len(token_ids)>0: \n",
        "            tokens = self.convert_ids_to_tokens(token_ids)                \n",
        "            return self.convert_tokens_to_string(tokens)\n",
        "        return \"\"\n",
        "\n",
        "    def __len__(self):          \n",
        "        return len(self._tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpEdDa_t15gN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = 'bert'\n",
        "lookup = Lookup(model)\n",
        "text = \"Daisy, Daisy, Give me your answer, do!\"\n",
        "print(\"\\n1. String to tokens (tokenize):\")\n",
        "tokens = lookup.tokenize(text)\n",
        "print(tokens)\n",
        "\n",
        "print(\"\\n2. Tokens to ints (convert_tokens_to_ids):\")\n",
        "ids = lookup.convert_tokens_to_ids(tokens)\n",
        "print(ids)\n",
        "        \n",
        "print(\"\\n2.5 Token to int (convert_tokens_to_ids with a single str):\")\n",
        "id = lookup.convert_tokens_to_ids(tokens[0])\n",
        "print(id)\n",
        "\n",
        "print(\"\\n3. Ints to tokens (convert_ids_to_tokens):\")\n",
        "tokens = lookup.convert_ids_to_tokens(ids)\n",
        "print(tokens)\n",
        "\n",
        "print(\"\\n3.5 Int to token (convert_ids_to_tokens with a single int):\")\n",
        "token = lookup.convert_ids_to_tokens(id)\n",
        "print(token)\n",
        "\n",
        "print(\"\\n4. Tokens to string (convert_tokens_to_string):\")\n",
        "recreated_text = lookup.convert_tokens_to_string(tokens)\n",
        "print(recreated_text)\n",
        "\n",
        "print(\"\\n5. String to ints (encode):\")\n",
        "ids = lookup.encode(text)\n",
        "print(ids)\n",
        "\n",
        "print(\"\\n6. Ints to string (decode):\")\n",
        "recreated_text = lookup.decode(ids)\n",
        "print(recreated_text)\n",
        "\n",
        "print(\"\\n7. Encode adding special tokens:\")\n",
        "ids = lookup.encode(text, add_bos_eos_tokens=True)\n",
        "print(ids)\n",
        "print(\"How it looks like with tokens: {}\".format(lookup.convert_ids_to_tokens(ids)))\n",
        "    \n",
        "print(\"\\n8. Decode skipping special tokens:\")\n",
        "recreated_text = lookup.decode(ids, skip_bos_eos_tokens=True)\n",
        "print(recreated_text)\n",
        "\n",
        "print(\"\\n9. Vocabulary size:\")\n",
        "vocab_size = lookup.__len__()\n",
        "print(vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CbgcLZA2_EB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, sys, json, random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch as nn\n",
        "import torch.utils.data\n",
        "\n",
        "from functools import partial"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFxEWLOD3BVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP7DLPeT3CZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Ignore slots for now.\n",
        "#TODO: Figure out what to do with slots\n",
        "#Remember to change load from file X, y\n",
        "def loader(data_folder, batch_size, src_lookup, tgt_lookup, min_seq_len_X = 5, max_seq_len_X = 1000, min_seq_len_y = 5,\n",
        "           max_seq_len_y = 1000, MEI = \"\"):\n",
        "    MEI = MEI.replace(\" \",\"_\")\n",
        "    pad_id = tgt_lookup.convert_tokens_to_ids(tgt_lookup.pad_token)\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        MyDataset(data_folder, \"train\", min_seq_len_X, max_seq_len_X, min_seq_len_y, max_seq_len_y, MEI),\n",
        "        num_workers=0,\n",
        "        batch_size=batch_size,\n",
        "        collate_fn=partial(paired_collate_fn, padding_idx = pad_id),\n",
        "        shuffle=True)\n",
        "\n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        MyDataset(data_folder, \"dev\", min_seq_len_X, max_seq_len_X, min_seq_len_y, max_seq_len_y, MEI),\n",
        "        num_workers=0,\n",
        "        batch_size=batch_size,\n",
        "        collate_fn=partial(paired_collate_fn, padding_idx = pad_id))\n",
        "    \n",
        "    return train_loader, valid_loader\n",
        "\n",
        "def paired_collate_fn(insts, padding_idx):\n",
        "    # insts contains a batch_size number of (x, y) elements    \n",
        "    src_insts, tgt_insts = list(zip(*insts))   \n",
        "    \n",
        "    src_max_len = max(len(inst) for inst in src_insts) # determines max size for all examples\n",
        "    \n",
        "    src_seq_lengths = torch.tensor(list(map(len, src_insts)), dtype=torch.long)    \n",
        "    src_seq_tensor = torch.tensor(np.array( [ inst + [padding_idx] * (src_max_len - len(inst)) for inst in src_insts ] ), dtype=torch.long)\n",
        "    src_seq_mask = torch.tensor(np.array( [ [1] * len(inst) + [0] * (src_max_len - len(inst)) for inst in src_insts ] ), dtype=torch.long)\n",
        "    \n",
        "    src_seq_lengths, perm_idx = src_seq_lengths.sort(0, descending=True)\n",
        "    src_seq_tensor = src_seq_tensor[perm_idx]   \n",
        "    src_seq_mask = src_seq_mask[perm_idx]\n",
        "    tgt_max_len = max(len(inst) for inst in tgt_insts)\n",
        "    \n",
        "    tgt_seq_lengths = torch.tensor(list(map(len, tgt_insts)), dtype=torch.long)    \n",
        "    tgt_seq_tensor = torch.tensor(np.array( [ inst + [padding_idx] * (tgt_max_len - len(inst)) for inst in tgt_insts ] ), dtype=torch.long)\n",
        "    tgt_seq_mask = torch.tensor(np.array( [ [1] * len(inst) + [0] * (tgt_max_len - len(inst)) for inst in tgt_insts ] ), dtype=torch.long)\n",
        "    \n",
        "    tgt_seq_lengths = tgt_seq_lengths[perm_idx]\n",
        "    tgt_seq_tensor = tgt_seq_tensor[perm_idx]      \n",
        "    tgt_seq_mask = tgt_seq_mask[perm_idx]   \n",
        "      \n",
        "    return ((src_seq_tensor, src_seq_lengths, src_seq_mask), (tgt_seq_tensor, tgt_seq_lengths, tgt_seq_mask))   \n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, type, min_seq_len_X, max_seq_len_X, min_seq_len_y, max_seq_len_y, MEI):  \n",
        "        self.root_dir = root_dir\n",
        "\n",
        "        self.X = [] # this will store joined sentences\n",
        "        self.y = [] # this will store the output\n",
        "\n",
        "    \n",
        "        with open(os.path.join(root_dir, type, MEI + '_output.txt'), 'r') as f:\n",
        "            y = [lookup.encode(y.strip(), add_bos_eos_tokens=True)  for y in f]\n",
        "        with open(os.path.join(root_dir, type, MEI + '_sentences.txt'), 'r') as g:\n",
        "            X = [lookup.encode(x.strip(), add_bos_eos_tokens=True)  for x in g]   \n",
        "                    \n",
        "        cut_over_X = 0\n",
        "        cut_under_X = 0\n",
        "        cut_over_y = 0\n",
        "        cut_under_y = 0\n",
        "        \n",
        "        # max len\n",
        "        for (sx, sy) in zip(X, y):\n",
        "            if len(sx) > max_seq_len_X:\n",
        "                cut_over_X += 1\n",
        "            elif len(sx) < min_seq_len_X+2:                \n",
        "                cut_under_X += 1\n",
        "            elif len(sy) > max_seq_len_y:\n",
        "                cut_over_y += 1\n",
        "            elif len(sy) < min_seq_len_y+2:                \n",
        "                cut_under_y += 1\n",
        "            else:\n",
        "                self.X.append(sx)\n",
        "                self.y.append(sy)         \n",
        "\n",
        "        c = list(zip(self.X, self.y))\n",
        "        random.shuffle(c)\n",
        "        self.X, self.y = zip(*c)\n",
        "        self.X = list(self.X)\n",
        "        self.y = list(self.y)\n",
        "                    \n",
        "        print(\"Dataset [{}] loaded with {} out of {} ({}%) instances.\".format(type, len(self.X), len(X), float(100.*len(self.X)/len(X)) ) )\n",
        "        print(\"\\t\\t For X, {} are over max_len {} and {} are under min_len {}.\".format(cut_over_X, max_seq_len_X, cut_under_X, min_seq_len_X))\n",
        "        print(\"\\t\\t For y, {} are over max_len {} and {} are under min_len {}.\".format(cut_over_y, max_seq_len_y, cut_under_y, min_seq_len_y))\n",
        "        \n",
        "        assert(len(self.X)==len(self.y))\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):        \n",
        "        return self.X[idx], self.y[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aSxtTi2BsIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_path = 'drive/My Drive/nlg/tiny'\n",
        "src_lookup = Lookup(model)\n",
        "tgt_lookup = Lookup(model)\n",
        "batch_size = 2\n",
        "min_seq_len_X = 10\n",
        "max_seq_len_X = 1000\n",
        "min_seq_len_y = min_seq_len_X\n",
        "max_seq_len_y = max_seq_len_X \n",
        "MEI = \"Management Overview\"\n",
        "model = 'bert'\n",
        "lookup = Lookup(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSZIQKi7B61Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, sys\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertConfig\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, device):       \n",
        "        super().__init__()\n",
        "        \n",
        "        self.hidden_size = 768\n",
        "\n",
        "        configuration = BertConfig()\n",
        "        configuration.output_attentions = True\n",
        "\n",
        "        self.bertmodel = BertModel(configuration)\n",
        "        self.bertmodel.resize_token_embeddings(vocab_size)      \n",
        "        for param in self.bertmodel.parameters():\n",
        "            param.requires_grad = False\n",
        "        \n",
        "        self.device = device\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, input_tuple):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_tuple (tensor): The input of the encoder. On the first position it must be a 2-D tensor of integers, padded. The second is the lenghts of the first.\n",
        "                Shape: ([batch_size, seq_len_enc], [batch_size], [att_mask]])\n",
        "\n",
        "        Returns:\n",
        "            Output shape: [batch_size, seq_len_enc, 768]\n",
        "            (tuple) Past shape: ((2, batch_size, num_heads, sequence_length, embed_size_per_head),(2, batch_size, num_heads, sequence_length, embed_size_per_head))\n",
        "            (tuple) Att shape: ((batch_size, num_heads, sequence_length, sequence_length), (batch_size, num_heads, sequence_length, sequence_length))\n",
        "\n",
        "        \n",
        "        \"\"\"\n",
        "        self.bertmodel.eval()\n",
        "        X, X_lengths, X_att_mask = input_tuple[0], input_tuple[1], input_tuple[2]\n",
        "        batch_size = X.size(0)\n",
        "        seq_len = X.size(1)\n",
        "        print(seq_len)\n",
        "        \n",
        "        output = torch.zeros(batch_size, seq_len, self.hidden_size).to(self.device)\n",
        "        output.requires_grad = False\n",
        "\n",
        "        \n",
        "        with torch.no_grad(): \n",
        "            hidden_states, past, att   = self.bertmodel(X, attention_mask = X_att_mask)  \n",
        "            for i in range(batch_size):\n",
        "                output[i:i+1, 0:X_lengths[i], :] = hidden_states[i:i+1, 0:X_lengths[i], :]\n",
        "            \n",
        "        return {'output':output, 'past':past, 'att': att}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFP-3FtGDdGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader, valid_loader = loader(data_path, batch_size, src_lookup, tgt_lookup, min_seq_len_X, max_seq_len_X, min_seq_len_y, max_seq_len_y, MEI = MEI)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx1otv-eB7VJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertModel, BertConfig\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, vocab_size, device = device):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "\n",
        "        configuration = BertConfig()\n",
        "        configuration.is_decoder = True\n",
        "        configuration.output_attentions = True\n",
        "\n",
        "        self.bertmodel = BertModel(configuration)\n",
        "        self.bertmodel.resize_token_embeddings(vocab_size) #resize the size of vocab to include new tokens \n",
        "        for param in self.bertmodel.parameters():\n",
        "            param.requires_grad = False\n",
        "        \n",
        "        self.lin_out = nn.Linear(hidden_size, vocab_size)\n",
        "        self.softmax = nn.LogSoftmax(dim = 1)\n",
        "\n",
        "        self.device = device\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, y_tuple, X_att_mask, encoder_hidden_states):\n",
        "        y = y_tuple[0]\n",
        "        y_lenghts = y_tuple[1]\n",
        "        y_att_mask = y_tuple[2]\n",
        "        batch_size = y.size(0)\n",
        "        y_seq_len = y.size(1)\n",
        "\n",
        "        output = torch.zeros(batch_size, y_seq_len, self.hidden_size).to(self.device)\n",
        "\n",
        "        output.requires_grad = False\n",
        "        with torch.no_grad():\n",
        "            hidden, past, decoder_attention = self.bertmodel(y, attention_mask = y_att_mask, \n",
        "                                                             encoder_hidden_states = encoder_hidden_states)  \n",
        "            for i in range(batch_size):\n",
        "                output[i:i+1, 0:y_lenghts[i], :] = hidden[i:i+1, 0:y_lenghts[i], :]\n",
        "\n",
        "        out_lin = self.lin_out(output)\n",
        "        output = self.softmax(out_lin)\n",
        "\n",
        "        return {'output':output, 'past':past, 'att': decoder_attention}     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4tZatHahyux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self, src_lookup, tgt_lookup, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        if torch.cuda.is_available():            \n",
        "            self.cuda = True\n",
        "            self.device = torch.device('cuda')\n",
        "        else:            \n",
        "            self.cuda = False\n",
        "            self.device = torch.device('cpu')\n",
        "\n",
        "        self.src_lookup = src_lookup\n",
        "        self.tgt_lookup = tgt_lookup\n",
        "        self.src_bos_token_id = src_lookup.convert_tokens_to_ids(src_lookup.bos_token)\n",
        "        self.src_eos_token_id = src_lookup.convert_tokens_to_ids(src_lookup.eos_token)\n",
        "        self.tgt_bos_token_id = src_lookup.convert_tokens_to_ids(tgt_lookup.bos_token)\n",
        "        self.tgt_eos_token_id = src_lookup.convert_tokens_to_ids(tgt_lookup.eos_token)\n",
        "    \n",
        "        self.encoder = encoder       \n",
        "        self.decoder = decoder\n",
        "        \n",
        "        self.device = device\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, X_tuple, y_tuple, teacher_forcing_ratio=0.):\n",
        "        x, x_lenghts, x_mask= X_tuple[0], X_tuple[1], X_tuple[2]\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        encoder_dict = self.encoder.forward((x, x_lenghts, x_mask))\n",
        "        enc_output = encoder_dict[\"output\"]\n",
        "        enc_past = encoder_dict[\"past\"]\n",
        "        enc_att = encoder_dict[\"att\"]\n",
        "\n",
        "        decoder_dict = self.decoder.forward(y_tuple, X_att_mask = x_mask, encoder_hidden_states = encoder_out['output'])\n",
        "\n",
        "        output_decoder = decoder_dict['output']\n",
        "        attention_decoder = decoder_dict['att']\n",
        "\n",
        "\n",
        "        return output_decoder, attention_decoder\n",
        "\n",
        "    def run_batch(self, X_tuple, y_tuple = None, criterion = None):\n",
        "        y = y_tuple[0]\n",
        "        print(\"Run batch {}\".format(y.shape))\n",
        "\n",
        "        output_decoder, attention_decoder = self.forward(X_tuple, y_tuple, teacher_forcing_ratio=0.)\n",
        "        print(\"Decoder out {}\".format(output_decoder.shape))\n",
        "\n",
        "        total_loss = 0\n",
        "\n",
        "        if criterion is not None:\n",
        "\n",
        "            loss = criterion(output_decoder.view(-1, vocab_size), y.contiguous().flatten())\n",
        "            print(\"Loss {}\".format(loss))\n",
        "            total_loss += loss\n",
        "\n",
        "        return output_decoder, total_loss, attention_decoder\n",
        "\n",
        "\n",
        "    def load_checkpoint(self, folder, extension):\n",
        "        filename = os.path.join(folder, \"checkpoint.\" + extension)\n",
        "        print(\"Loading model {} ...\".format(filename))\n",
        "        if not os.path.exists(filename):\n",
        "            print(\"\\tModel file not found, not loading anything!\")\n",
        "            #return {}\n",
        "            raise Exception(\"Error, model file not found! {} -> model {}\".format(folder, extension))\n",
        "\n",
        "        checkpoint = torch.load(filename, map_location=self.device)\n",
        "        self.load_state_dict(checkpoint[\"state_dict\"])\n",
        "        \n",
        "        self.encoder.to(self.device)\n",
        "        self.decoder.to(self.device)\n",
        "        return checkpoint[\"extra\"]\n",
        "\n",
        "    def save_checkpoint(self, folder, extension, extra={}):\n",
        "        filename = os.path.join(folder, \"checkpoint.\" + extension)\n",
        "        checkpoint = {}\n",
        "        checkpoint[\"state_dict\"] = self.state_dict()\n",
        "        checkpoint[\"extra\"] = extra\n",
        "        torch.save(checkpoint, filename)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scXcoGI7_P8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(vocab_size=vocab_size, device = device)\n",
        "decoder = Decoder(hidden_size=hidden_size, vocab_size=vocab_size, device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjHbm_BF_fA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src_lookup = Lookup('bert')\n",
        "tgt_lookup = Lookup('bert')\n",
        "model = EncoderDecoder(src_lookup = src_lookup, tgt_lookup = tgt_lookup, encoder = encoder , decoder = decoder, device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9MeK13i8Blq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_sequences(sequences, lookup):\n",
        "    \"\"\"\n",
        "        Cleans BOS and EOS from sequences.\n",
        "        sequences (list): is a list of lists containing ints corresponding to the lookup\n",
        "    \"\"\"\n",
        "    bos_id = lookup.convert_tokens_to_ids(lookup.bos_token)\n",
        "    eos_id = lookup.convert_tokens_to_ids(lookup.eos_token)\n",
        "    cleaned_sequences = []        \n",
        "    for seq in sequences:\n",
        "        lst = []\n",
        "        for i, value in enumerate(seq):                                \n",
        "            if i == 0 and value == bos_id: # skip bos\n",
        "                continue\n",
        "            if i>0 and value == eos_id: # stop before first eos       \n",
        "                break\n",
        "            lst.append(value)\n",
        "        cleaned_sequences.append(lst)\n",
        "    \n",
        "    return cleaned_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PliYqXS3Edvi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train\n",
        "patience = 10\n",
        "max_epochs=40\n",
        "current_patience = patience\n",
        "current_epoch = 0\n",
        "print(str(current_epoch))\n",
        "while current_patience > 0 and current_epoch < max_epochs:  \n",
        "    model.train()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, amsgrad=True)#, weight_decay=1e-3) \n",
        "    criterion = nn.NLLLoss()\n",
        "    total_loss = 0\n",
        "    t = tqdm(train_loader, mininterval=0.5, desc=\"Epoch \" + str(current_epoch)+\" [train]\", unit=\"b\")\n",
        "    for i, t in enumerate(t):\n",
        "        X_tuple = t[0]\n",
        "        y_tuple = t[1] \n",
        "        output_decoder, loss, attention_decoder = model.run_batch(X_tuple, y_tuple, criterion = criterion)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)    \n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    #dev\n",
        "    model.eval()\n",
        "    seq_len = 2\n",
        "    with torch.no_grad():\n",
        "        y_gold = list()\n",
        "        y_predicted = list()\n",
        "        t = tqdm(valid_loader, mininterval=0.5, desc=\"Epoch \" + str(current_epoch)+\" [valid]\", unit=\"b\")\n",
        "        for i, v in enumerate(t):\n",
        "            X_tuple = v[0]\n",
        "            y_tuple = v[1]\n",
        "\n",
        "            output_decoder, loss, attention_decoder = model.run_batch(X_tuple, y_tuple, criterion = criterion)\n",
        "            y_predicted_batch = output_decoder.argmax(dim=2)\n",
        "            print(y_predicted_batch.shape)\n",
        "            y_gold += y_tuple[0].tolist()\n",
        "\n",
        "            y_predicted += y_predicted_batch.tolist()\n",
        "\n",
        "        for i in range(seq_len):\n",
        "\n",
        "            lst = []\n",
        "            for j in range(len(y_predicted[i])):\n",
        "                lst.append(y_predicted[i][j])\n",
        "\n",
        "            print(\"Y Pred: \")\n",
        "            tstr = tgt_lookup.decode(lst, skip_bos_eos_tokens = True)\n",
        "            print(tstr)\n",
        "\n",
        "            glst = []\n",
        "            print(\"Y Gold: \")\n",
        "            for g in range(len(y_gold[i])):\n",
        "                glst.append(y_gold[i][g])\n",
        "            gstr = tgt_lookup.decode(glst, skip_bos_eos_tokens = True)\n",
        "            print(gstr)\n",
        "    current_epoch += 1\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHaWuF-UCnh9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}